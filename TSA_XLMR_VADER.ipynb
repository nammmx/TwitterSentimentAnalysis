{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XWAdCMYaDMIf",
        "SrTL6d80DO-9",
        "XxROKIzrDVNX",
        "iS2yhuIRDYpr",
        "II3ARD16ESLZ",
        "RpJibhYNE3Fq",
        "mkPkobP_FbzF",
        "-AOOvn5lGTbf",
        "tWUDVcZ6DfAu",
        "y7CZE3CZHV22",
        "CdTnb2UaHV24",
        "Bw_GU5skHF86",
        "BW43pMlYHlaJ",
        "YAZy_qE1Ienu",
        "g5HmbQE2JDX4",
        "H9uxSVZJKtFB",
        "GXpHf0PxLrvD",
        "WW_psWXNTtkn"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4973e2dc5cf24327911442e1a0cdaab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54ffeacc0d6447ef8f82a2221a0f5bfc",
              "IPY_MODEL_3998596af864468db6b6076aec544b49",
              "IPY_MODEL_14aaf89973104cd58cbb5c6b749bec6e"
            ],
            "layout": "IPY_MODEL_75527beca0a34825ab715721583ee4d9"
          }
        },
        "54ffeacc0d6447ef8f82a2221a0f5bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca19b4fda90b43e699dbb4a4da73501f",
            "placeholder": "​",
            "style": "IPY_MODEL_4bf2f493de6a492ca70dced9c5c58202",
            "value": "Map: 100%"
          }
        },
        "3998596af864468db6b6076aec544b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cc5d058ca21431f8fa06fecf5b7d4ac",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbc0fdebf2f746b38f72489754d04919",
            "value": 8000
          }
        },
        "14aaf89973104cd58cbb5c6b749bec6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14cb7a122c744252b6d5aa2d96b26b44",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb2d65a46ed4214989f0bc36437aca9",
            "value": " 8000/8000 [00:01&lt;00:00, 6425.27 examples/s]"
          }
        },
        "75527beca0a34825ab715721583ee4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ca19b4fda90b43e699dbb4a4da73501f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf2f493de6a492ca70dced9c5c58202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cc5d058ca21431f8fa06fecf5b7d4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc0fdebf2f746b38f72489754d04919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14cb7a122c744252b6d5aa2d96b26b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb2d65a46ed4214989f0bc36437aca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b6bf7480af40618f60f7c7235777e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7870c888b9a948a98158aa1d554b9fad",
              "IPY_MODEL_69d1322d14cf4108a65c2f188f933775",
              "IPY_MODEL_7988d53aa993475e99f9f2644ce6362e"
            ],
            "layout": "IPY_MODEL_62f6601ff87542329d18acc2bf99aba8"
          }
        },
        "7870c888b9a948a98158aa1d554b9fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff02d879084f436d993b7cfb6335e980",
            "placeholder": "​",
            "style": "IPY_MODEL_a6cbfd5bede3490e99c3e58a645d84e1",
            "value": "Map: 100%"
          }
        },
        "69d1322d14cf4108a65c2f188f933775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce991c76749742cdb110a732b24c6282",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41b96e3cce204359853767f32ac6f2ab",
            "value": 1000
          }
        },
        "7988d53aa993475e99f9f2644ce6362e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc30c586b1e5408ebf958e2219dc443e",
            "placeholder": "​",
            "style": "IPY_MODEL_6cefaea9620a4479a9847969e8cee21e",
            "value": " 1000/1000 [00:00&lt;00:00, 6467.87 examples/s]"
          }
        },
        "62f6601ff87542329d18acc2bf99aba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ff02d879084f436d993b7cfb6335e980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6cbfd5bede3490e99c3e58a645d84e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce991c76749742cdb110a732b24c6282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b96e3cce204359853767f32ac6f2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc30c586b1e5408ebf958e2219dc443e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cefaea9620a4479a9847969e8cee21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping"
      ],
      "metadata": {
        "id": "XWAdCMYaDMIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# define the maximum number of tweets\n",
        "maxTweets = 1000000\n",
        "# define start and end date\n",
        "start = '2019-01-01'\n",
        "end = '2023-02-04'\n",
        "\n",
        "####################################################################\n",
        "# Keyword: alternative meat\n",
        "####################################################################\n",
        "# create empty list, append tweets to this list from below for-loop\n",
        "tweet_list = []\n",
        "keyword = 'alternative meat'\n",
        "\n",
        "# for-loop to retreive tweets and append to tweet_list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{end} exclude:replies').get_items()):\n",
        "    if i>maxTweets:\n",
        "        break\n",
        "    tweet_list.append([tweet.id,tweet.date,tweet.user.location,tweet.rawContent,tweet.likeCount,tweet.replyCount, tweet.retweetCount, tweet.quoteCount, tweet.retweetedTweet, tweet.quotedTweet,tweet.sourceLabel,tweet.user.username, tweet.lang])\n",
        "\n",
        "# create a dataframe from tweet_list\n",
        "tweets_df_alternativemeat = pd.DataFrame(tweet_list, columns=[\"tweetid\",\"datecreated\",\"locationuser\",\"tweet\",\"number_of_likes\",\"count_of_replies\",\"count_of_retweets\",\"count_of_qoutedusers\",\"retweetedtweet\", \"qoutedTweet\",\"source_of_tweet\",\"User\",\"language\"])\n",
        "\n",
        "# only keep tweets with the occurence of the following phrases\n",
        "searchfor = ['meat alternative', 'alternative meat', 'meatalternative', 'alternativemeat']\n",
        "result_alternativemeat = tweets_df_alternativemeat[tweets_df_alternativemeat['tweet'].str.contains('|'.join(searchfor), case=False)]\n",
        "\n",
        "# export dataframe\n",
        "result_alternativemeat.to_json('alternativemeat.json', orient='records', lines=True, date_format='iso')\n",
        "\n",
        "####################################################################\n",
        "# Keyword: alternative protein\n",
        "####################################################################\n",
        "# create empty list, append tweets to this list from below for-loop\n",
        "tweet_list = []\n",
        "keyword = 'alternative protein'\n",
        "\n",
        "# for-loop to retreive tweets and append to tweet_list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{end} exclude:replies').get_items()):\n",
        "    if i>maxTweets:\n",
        "        break\n",
        "    tweet_list.append([tweet.id,tweet.date,tweet.user.location,tweet.rawContent,tweet.likeCount,tweet.replyCount, tweet.retweetCount, tweet.quoteCount, tweet.retweetedTweet, tweet.quotedTweet,tweet.sourceLabel,tweet.user.username, tweet.lang])\n",
        "\n",
        "# create a dataframe from tweet_list\n",
        "tweets_df_alternativeprotein = pd.DataFrame(tweet_list, columns=[\"tweetid\",\"datecreated\",\"locationuser\",\"tweet\",\"number_of_likes\",\"count_of_replies\",\"count_of_retweets\",\"count_of_qoutedusers\",\"retweetedtweet\", \"qoutedTweet\",\"source_of_tweet\",\"User\",\"language\"])\n",
        "\n",
        "# only keep tweets with the occurence of the following phrases\n",
        "searchfor = ['protein alternative', 'alternative protein', 'proteinalternative', 'alternativeprotein']\n",
        "result_alternativeprotein = tweets_df_alternativeprotein[tweets_df_alternativeprotein['tweet'].str.contains('|'.join(searchfor), case=False)]\n",
        "\n",
        "# export dataframe\n",
        "result_alternativeprotein.to_json('alternativeprotein.json', orient='records', lines=True, date_format='iso')\n",
        "\n",
        "####################################################################\n",
        "# Keyword: beyond meat\n",
        "####################################################################\n",
        "# create empty list, append tweets to this list from below for-loop\n",
        "tweet_list = []\n",
        "keyword = 'beyond meat'\n",
        "\n",
        "# for-loop to retreive tweets and append to tweet_list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{end} exclude:replies').get_items()):\n",
        "    if i>maxTweets:\n",
        "        break\n",
        "    tweet_list.append([tweet.id,tweet.date,tweet.user.location,tweet.rawContent,tweet.likeCount,tweet.replyCount, tweet.retweetCount, tweet.quoteCount, tweet.retweetedTweet, tweet.quotedTweet,tweet.sourceLabel,tweet.user.username, tweet.lang])\n",
        "\n",
        "# create a dataframe from tweet_list\n",
        "tweets_df_beyondmeat = pd.DataFrame(tweet_list, columns=[\"tweetid\",\"datecreated\",\"locationuser\",\"tweet\",\"number_of_likes\",\"count_of_replies\",\"count_of_retweets\",\"count_of_qoutedusers\",\"retweetedtweet\", \"qoutedTweet\",\"source_of_tweet\",\"User\",\"language\"])\n",
        "\n",
        "# only keep tweets with the occurence of the following phrases\n",
        "searchfor = ['beyond meat', 'beyondmeat']\n",
        "result_beyondmeat = tweets_df_beyondmeat[tweets_df_beyondmeat['tweet'].str.contains('|'.join(searchfor), case=False)]\n",
        "\n",
        "# export dataframe\n",
        "result_beyondmeat.to_json('beyondmeat.json', orient='records', lines=True, date_format='iso')\n",
        "\n",
        "####################################################################\n",
        "# Keyword: impossible foods\n",
        "####################################################################\n",
        "# create empty list, append tweets to this list from below for-loop\n",
        "tweet_list = []\n",
        "keyword = 'impossible foods'\n",
        "\n",
        "# for-loop to retreive tweets and append to tweet_list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{end} exclude:replies').get_items()):\n",
        "    if i>maxTweets:\n",
        "        break\n",
        "    tweet_list.append([tweet.id,tweet.date,tweet.user.location,tweet.rawContent,tweet.likeCount,tweet.replyCount, tweet.retweetCount, tweet.quoteCount, tweet.retweetedTweet, tweet.quotedTweet,tweet.sourceLabel,tweet.user.username, tweet.lang])\n",
        "\n",
        "# create a dataframe from tweet_list\n",
        "tweets_df_impossiblefoods = pd.DataFrame(tweet_list, columns=[\"tweetid\",\"datecreated\",\"locationuser\",\"tweet\",\"number_of_likes\",\"count_of_replies\",\"count_of_retweets\",\"count_of_qoutedusers\",\"retweetedtweet\", \"qoutedTweet\",\"source_of_tweet\",\"User\",\"language\"])\n",
        "\n",
        "# only keep tweets with the occurence of the following phrases\n",
        "searchfor = ['impossible foods', 'impossiblefoods', 'impossible burger']\n",
        "result_impossiblefoods = tweets_df_impossiblefoods[tweets_df_impossiblefoods['tweet'].str.contains('|'.join(searchfor), case=False)]\n",
        "\n",
        "# export dataframe\n",
        "result_impossiblefoods.to_json('impossiblefoods.json', orient='records', lines=True, date_format='iso')\n",
        "\n",
        "####################################################################\n",
        "# Keyword: plant based meat\n",
        "####################################################################\n",
        "# create empty list, append tweets to this list from below for-loop\n",
        "tweet_list = []\n",
        "keyword = 'plant based meat'\n",
        "\n",
        "# for-loop to retreive tweets and append to tweet_list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{end} exclude:replies').get_items()):\n",
        "    if i>maxTweets:\n",
        "        break\n",
        "    tweet_list.append([tweet.id,tweet.date,tweet.user.location,tweet.rawContent,tweet.likeCount,tweet.replyCount, tweet.retweetCount, tweet.quoteCount, tweet.retweetedTweet, tweet.quotedTweet,tweet.sourceLabel,tweet.user.username, tweet.lang])\n",
        "\n",
        "# create a dataframe from tweet_list\n",
        "tweets_df_plantbasedmeat = pd.DataFrame(tweet_list, columns=[\"tweetid\",\"datecreated\",\"locationuser\",\"tweet\",\"number_of_likes\",\"count_of_replies\",\"count_of_retweets\",\"count_of_qoutedusers\",\"retweetedtweet\", \"qoutedTweet\",\"source_of_tweet\",\"User\",\"language\"])\n",
        "\n",
        "# only keep tweets with the occurence of the following phrases\n",
        "searchfor = ['plant based meat', 'plant-based meat']\n",
        "result_plantbasedmeat = tweets_df_plantbasedmeat[tweets_df_plantbasedmeat['tweet'].str.contains('|'.join(searchfor), case=False)]\n",
        "\n",
        "# export dataframe\n",
        "result_plantbasedmeat.to_json('plantbasedmeat.json', orient='records', lines=True, date_format='iso')\n",
        "\n",
        "####################################################################\n",
        "# Keyword: fake meat\n",
        "####################################################################\n",
        "# create empty list, append tweets to this list from below for-loop\n",
        "tweet_list = []\n",
        "keyword = 'fake meat'\n",
        "\n",
        "# for-loop to retreive tweets and append to tweet_list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} since:{start} until:{end} exclude:replies').get_items()):\n",
        "    if i>maxTweets:\n",
        "        break\n",
        "    tweet_list.append([tweet.id,tweet.date,tweet.user.location,tweet.rawContent,tweet.likeCount,tweet.replyCount, tweet.retweetCount, tweet.quoteCount, tweet.retweetedTweet, tweet.quotedTweet,tweet.sourceLabel,tweet.user.username, tweet.lang])\n",
        "\n",
        "# create a dataframe from tweet_list\n",
        "tweets_df_fakemeat = pd.DataFrame(tweet_list, columns=[\"tweetid\",\"datecreated\",\"locationuser\",\"tweet\",\"number_of_likes\",\"count_of_replies\",\"count_of_retweets\",\"count_of_qoutedusers\",\"retweetedtweet\", \"qoutedTweet\",\"source_of_tweet\",\"User\",\"language\"])\n",
        "\n",
        "# only keep tweets with the occurence of the following phrases\n",
        "searchfor = ['fake meat', 'fakemeat', 'fake-meat']\n",
        "result_fakemeat = tweets_df_fakemeat[tweets_df_fakemeat['tweet'].str.contains('|'.join(searchfor), case=False)]\n",
        "\n",
        "# export dataframe\n",
        "result_fakemeat.to_json('fakemeat.json', orient='records', lines=True, date_format='iso')\n"
      ],
      "metadata": {
        "id": "oxVo5_PgDmGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "SrTL6d80DO-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and add keyword column\n",
        "df_alternativeprotein = pd.read_json('alternativeprotein.json', lines=True)\n",
        "df_alternativeprotein['keyword'] = 'alternativeprotein'\n",
        "df_alternativemeat = pd.read_json('alternativemeat.json', lines=True)\n",
        "df_alternativemeat['keyword'] = 'alternativemeat'\n",
        "df_fakemeat = pd.read_json('fakemeat.json', lines=True)\n",
        "df_fakemeat['keyword'] = 'fakemeat'\n",
        "df_beyondmeat = pd.read_json('beyondmeat.json', lines=True)\n",
        "df_beyondmeat['keyword'] = 'beyondmeat'\n",
        "df_impossiblefoods = pd.read_json('impossiblefoods.json', lines=True)\n",
        "df_impossiblefoods['keyword'] = 'impossiblefoods'\n",
        "df_plantbasedmeat = pd.read_json('plantbasedmeat.json', lines=True)\n",
        "df_plantbasedmeat['keyword'] = 'plantbasedmeat'\n",
        "\n",
        "# list of dataframes\n",
        "datasets = [df_alternativeprotein, df_alternativemeat, df_fakemeat, df_beyondmeat, df_impossiblefoods, df_plantbasedmeat]\n",
        "# create combined data set\n",
        "df_tweets = pd.concat(datasets, ignore_index=True)\n",
        "\n",
        "# drop duplicates in 'tweetid' and 'tweet'\n",
        "df_tweets = df_tweets.drop_duplicates(\"tweetid\")\n",
        "df_tweets = df_tweets.drop_duplicates(\"tweet\")\n",
        "\n",
        "# only keep english, spanish, german, french tweets\n",
        "df_tweets = df_tweets[(df_tweets['language']==\"en\") | (df_tweets['language']==\"es\") | (df_tweets['language']==\"de\") | (df_tweets['language']==\"fr\")]\n",
        "\n",
        "# tweet count by keyword\n",
        "print(df_tweets['keyword'].value_counts())\n",
        "# tweet count by language\n",
        "print(df_tweets['language'].value_counts())\n",
        "# tweet count by year\n",
        "print(df_tweets.groupby(pd.DatetimeIndex(df_tweets['datecreated']).year).count().sort_values(\"tweetid\", ascending=False).iloc[:,:1])"
      ],
      "metadata": {
        "id": "4iLBN5cADqhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing and Sampling for Annotation"
      ],
      "metadata": {
        "id": "XxROKIzrDVNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df_alternativeprotein = pd.read_json('alternativeprotein.json', lines=True)\n",
        "df_alternativemeat = pd.read_json('alternativemeat.json', lines=True)\n",
        "df_fakemeat = pd.read_json('fakemeat.json', lines=True)\n",
        "df_beyondmeat = pd.read_json('beyondmeat.json', lines=True)\n",
        "df_impossiblefoods = pd.read_json('impossiblefoods.json', lines=True)\n",
        "df_plantbasedmeat = pd.read_json('plantbasedmeat.json', lines=True)\n",
        "\n",
        "# remove unnecessary columns\n",
        "df_alternativeprotein.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "df_alternativemeat.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "df_fakemeat.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "df_beyondmeat.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "df_impossiblefoods.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "df_plantbasedmeat.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "\n",
        "# sample 10k from each keyword to create a subset due to imbalanced distribution of tweets per keyword\n",
        "df_alternativeprotein_sample = df_alternativeprotein.sample(n=10000)\n",
        "df_alternativemeat_sample = df_alternativemeat.sample(n=10000)\n",
        "df_fakemeat_sample = df_fakemeat.sample(n=10000)\n",
        "df_beyondmeat_sample = df_beyondmeat.sample(n=10000)\n",
        "df_impossiblefoods_sample = df_impossiblefoods.sample(n=10000)\n",
        "df_plantbasedmeat_sample = df_plantbasedmeat.sample(n=10000)\n",
        "\n",
        "# list of dataframes\n",
        "datasets = [df_alternativeprotein_sample, df_alternativemeat_sample, df_fakemeat_sample, df_beyondmeat_sample, df_impossiblefoods_sample, df_plantbasedmeat_sample]\n",
        "\n",
        "# create combined data set\n",
        "df_tweets = pd.concat(datasets, ignore_index=True)\n",
        "\n",
        "# drop duplicates in 'tweetid' and 'tweet'\n",
        "df_tweets = df_tweets.drop_duplicates(\"tweetid\")\n",
        "df_tweets = df_tweets.drop_duplicates(\"tweet\")\n",
        "\n",
        "\n",
        "# only keep English, Spanish, German, and French tweets\n",
        "df_tweets = df_tweets[(df_tweets['language']==\"en\") | (df_tweets['language']==\"es\") | (df_tweets['language']==\"de\") | (df_tweets['language']==\"fr\")]\n",
        "\n",
        "# each language\n",
        "df_english = df_tweets[df_tweets['language']=='en']\n",
        "df_spanish = df_tweets[df_tweets['language']=='es']\n",
        "df_german = df_tweets[df_tweets['language']=='de']\n",
        "df_french = df_tweets[df_tweets['language']=='fr']\n",
        "\n",
        "# sample of each language\n",
        "df_annotation_english_sample = df_english.sample(n=8500)\n",
        "df_annotation_spanish_sample = df_spanish.sample(n=500)\n",
        "df_annotation_german_sample = df_german.sample(n=500)\n",
        "df_annotation_french_sample = df_french.sample(n=500)\n",
        "\n",
        "#list of dataframes\n",
        "annotation = [df_annotation_english_sample, df_annotation_spanish_sample, df_annotation_german_sample, df_annotation_french_sample]\n",
        "\n",
        "# final annotaion dataset\n",
        "df_annotation1 = pd.concat(annotation, ignore_index=True)\n",
        "\n",
        "# export final annotation dataset to Excel\n",
        "df_annotation1.to_excel(\"annotation1.xlsx\", index=False, header=True)\n",
        "\n",
        "# export full dataset\n",
        "# load data and add keyword column\n",
        "df_alternativeprotein = pd.read_json('alternativeprotein.json', lines=True)\n",
        "df_alternativemeat = pd.read_json('alternativemeat.json', lines=True)\n",
        "df_fakemeat = pd.read_json('fakemeat.json', lines=True)\n",
        "df_beyondmeat = pd.read_json('beyondmeat.json', lines=True)\n",
        "df_impossiblefoods = pd.read_json('impossiblefoods.json', lines=True)\n",
        "df_plantbasedmeat = pd.read_json('plantbasedmeat.json', lines=True)\n",
        "\n",
        "# list of dataframes\n",
        "datasets = [df_alternativeprotein, df_alternativemeat, df_fakemeat, df_beyondmeat, df_impossiblefoods, df_plantbasedmeat]\n",
        "# create combined data set\n",
        "df_tweets = pd.concat(datasets, ignore_index=True)\n",
        "\n",
        "# drop duplicates in 'tweetid' and 'tweet'\n",
        "df_tweets = df_tweets.drop_duplicates(\"tweetid\")\n",
        "df_tweets = df_tweets.drop_duplicates(\"tweet\")\n",
        "\n",
        "# only keep english, spanish, german, french tweets\n",
        "df_tweets = df_tweets[(df_tweets['language']==\"en\") | (df_tweets['language']==\"es\") | (df_tweets['language']==\"de\") | (df_tweets['language']==\"fr\")]\n",
        "\n",
        "df_tweets.drop(columns=['retweetedtweet', 'source_of_tweet', 'count_of_qoutedusers', 'number_of_likes', 'count_of_replies', 'count_of_retweets', 'locationuser'], axis=1, inplace=True)\n",
        "\n",
        "df_tweets.to_csv(\"AltProteinTweets-All-2023.csv\", index=False, header=True, encoding='utf-8')"
      ],
      "metadata": {
        "id": "Zad8r4fnDufj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Krippendorff alpha inter-rater reliability"
      ],
      "metadata": {
        "id": "Ekx2csbaAHWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install krippendorff"
      ],
      "metadata": {
        "id": "BAu_eTULAIeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# whole dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import krippendorff\n",
        "dataset_rater = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/inter_rater.xlsx\"\n",
        "dataset_rater = pd.read_excel(dataset_rater)\n",
        "'''\n",
        "labels = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
        "dataset_rater['sentiment'] = dataset_rater['sentiment'].map(labels)\n",
        "dataset_rater['sentiment2'] = dataset_rater['sentiment2'].map(labels)\n",
        "'''\n",
        "dataset_rater.drop(columns=['tweetid', 'datecreated', 'tweet', 'qoutedTweet', 'User', 'language'], inplace=True)"
      ],
      "metadata": {
        "id": "fvRGLikJAf9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_string_1 = dataset_rater['sentiment'].to_csv(index=False, header=False, sep='\\t')\n",
        "df_string_2 = dataset_rater['sentiment2'].to_csv(index=False, header=False, sep='\\t')\n",
        "\n",
        "string_str = (df_string_1,\n",
        "          df_string_2)\n",
        "\n",
        "string = [[np.nan if v == \"*\" else int(v) for v in coder.split()] for coder in string_str]\n",
        "\n",
        "krippendorff.alpha(reliability_data=string, level_of_measurement='nominal')"
      ],
      "metadata": {
        "id": "RNnlsE_KAk_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER"
      ],
      "metadata": {
        "id": "iS2yhuIRDYpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The functions in this section are adapted from this article: https://towardsdatascience.com/social-media-sentiment-analysis-in-python-with-vader-no-training-required-4bc6a21e87b8"
      ],
      "metadata": {
        "id": "JKZi9nZzun3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages and load data"
      ],
      "metadata": {
        "id": "II3ARD16ESLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor sentencepiece vader-multi spacy huggingface_hub transformers==4.28.0 datasets optuna"
      ],
      "metadata": {
        "id": "wStKSI-PEbmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-Annotated-2023.xlsx\"\n",
        "df = pd.read_excel(dataset)"
      ],
      "metadata": {
        "id": "lZgbeo50D2nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "RpJibhYNE3Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df['tweet'] = df.apply(preprocess_tweets, axis=1)\n",
        "df.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df = df.rename(columns={'tweet': 'text', 'sentiment': 'label'})\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rwO3FtovE5OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sent_analyzer = SentimentIntensityAnalyzer()\n",
        "positive = str(df['text'].values[6388])\n",
        "negative = str(df['text'].values[2236])\n",
        "neutral = str(df['text'].values[5858])\n",
        "print(positive)\n",
        "print(neutral)\n",
        "print(negative)\n",
        "print()\n",
        "print(sent_analyzer.polarity_scores(positive))\n",
        "print(sent_analyzer.polarity_scores(negative))\n",
        "print(sent_analyzer.polarity_scores(neutral))\n",
        "print()\n",
        "print(sent_analyzer.polarity_scores('very good'))\n",
        "print(sent_analyzer.polarity_scores('VERY good'))\n",
        "print(sent_analyzer.polarity_scores('I do not understand the new fake meat fad. Highly process plant - how be that well than real meat?'))\n",
        "print(sent_analyzer.polarity_scores(\"I do not understand the new fake meat fad. Highly processed plants - how is that better than real meat?\"))"
      ],
      "metadata": {
        "id": "HMP75BEjE8SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test-Split and Functions"
      ],
      "metadata": {
        "id": "mkPkobP_FbzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define independent/dependent variable\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# split with stratify on language and label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pd.concat([X[\"language\"], y], axis=1), stratify=pd.concat([X[\"language\"], y], axis=1), test_size=0.1, random_state=5)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_test = y_test.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)"
      ],
      "metadata": {
        "id": "c3px_6YsE8Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
        "df_train['label'] = df_train['label'].map(labels)\n",
        "\n",
        "#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sent_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def format_output(output_dict):\n",
        "\n",
        "  polarity = \"neutral\"\n",
        "\n",
        "  if(output_dict['compound']>= 0.05):\n",
        "    polarity = \"positive\"\n",
        "\n",
        "  elif(output_dict['compound']<= -0.05):\n",
        "    polarity = \"negative\"\n",
        "\n",
        "  return polarity\n",
        "\n",
        "def predict_sentiment(text):\n",
        "\n",
        "  output_dict =  sent_analyzer.polarity_scores(text)\n",
        "  return format_output(output_dict)"
      ],
      "metadata": {
        "id": "C8t0WWBGE8XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "-AOOvn5lGTbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each language\n",
        "df_train_en = df_train[df_train[\"language\"]==\"en\"]\n",
        "df_train_es = df_train[df_train[\"language\"]==\"es\"]\n",
        "df_train_de = df_train[df_train[\"language\"]==\"de\"]\n",
        "df_train_fr = df_train[df_train[\"language\"]==\"fr\"]"
      ],
      "metadata": {
        "id": "D2-tSdkzE8Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. No Pre-processing"
      ],
      "metadata": {
        "id": "ESlGTigJF6w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# generate VADER predictions\n",
        "df_train_en[\"vader_prediction\"] = df_train_en[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# print results\n",
        "accuracy = accuracy_score(df_train_en['label'], df_train_en['vader_prediction'])\n",
        "\n",
        "print(\"Accuracy: {}\\n\".format(accuracy))\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_en['label'], df_train_en['vader_prediction'], digits=4))\n",
        "\n",
        "# reset\n",
        "df_train_en = df_train[df_train[\"language\"]==\"en\"]"
      ],
      "metadata": {
        "id": "wqJy6syHF9Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Lower-casing"
      ],
      "metadata": {
        "id": "_mVbEJcDGBG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_en['text'] = df_train_en['text'].apply(lambda x: x.lower())\n",
        "\n",
        "# generate VADER predictions\n",
        "df_train_en[\"vader_prediction\"] = df_train_en[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# print results\n",
        "accuracy = accuracy_score(df_train_en['label'], df_train_en['vader_prediction'])\n",
        "\n",
        "print(\"Accuracy: {}\\n\".format(accuracy))\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_en['label'], df_train_en['vader_prediction'], digits=4))\n",
        "\n",
        "# reset\n",
        "df_train_en = df_train[df_train[\"language\"]==\"en\"]"
      ],
      "metadata": {
        "id": "1DxtkKz7GBPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Lemmatization"
      ],
      "metadata": {
        "id": "MquQ0pJNGFpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# loading the english-language-model into object called nlp\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize(text):\n",
        "\n",
        "    # apply english-language-model to \"text\" and store it in \"doc\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # for each token (word) in \"doc\": lemmatize that token (\"token.lemma\"), and if that token is not a stop-word (\"token.is_stop\"),\n",
        "    # then join it into a sentence with spaces in-between (' '.join(...))\n",
        "    text_lemmatized = ' '.join([token.lemma_ for token in doc])\n",
        "    return text_lemmatized\n",
        "df_train_en['text'] = df_train_en['text'].apply(lemmatize)\n",
        "\n",
        "# generate VADER predictions\n",
        "df_train_en[\"vader_prediction\"] = df_train_en[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# print results\n",
        "accuracy = accuracy_score(df_train_en['label'], df_train_en['vader_prediction'])\n",
        "\n",
        "print(\"Accuracy: {}\\n\".format(accuracy))\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_en['label'], df_train_en['vader_prediction'], digits=4))\n",
        "\n",
        "# reset\n",
        "df_train_en = df_train[df_train[\"language\"]==\"en\"]"
      ],
      "metadata": {
        "id": "s9NzWDhfGFxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Lower-casing and Lemmatization"
      ],
      "metadata": {
        "id": "cxBKo2_eGLJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_en['text'] = df_train_en['text'].apply(lambda x: x.lower())\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize(text):\n",
        "\n",
        "    # apply english-language-model to \"text\" and store it in \"doc\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # for each token (word) in \"doc\": lemmatize that token (\"token.lemma\"), and if that token is not a stop-word (\"token.is_stop\"),\n",
        "    # then join it into a sentence with spaces in-between (' '.join(...))\n",
        "    text_lemmatized = ' '.join([token.lemma_ for token in doc])\n",
        "    return text_lemmatized\n",
        "df_train_en['text'] = df_train_en['text'].apply(lemmatize)\n",
        "\n",
        "# generate VADER predictions\n",
        "df_train_en[\"vader_prediction\"] = df_train_en[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# print results\n",
        "accuracy = accuracy_score(df_train_en['label'], df_train_en['vader_prediction'])\n",
        "\n",
        "print(\"Accuracy: {}\\n\".format(accuracy))\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_en['label'], df_train_en['vader_prediction'], digits=4))\n",
        "\n",
        "# reset\n",
        "df_train_en = df_train[df_train[\"language\"]==\"en\"]"
      ],
      "metadata": {
        "id": "fUF8U29OGLSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### English"
      ],
      "metadata": {
        "id": "L0A_LYVhGYmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate VADER predictions\n",
        "df_train_en[\"vader_prediction\"] = df_train_en[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_en['label'], df_train_en['vader_prediction'], digits=4))"
      ],
      "metadata": {
        "id": "HGiGKIoOGZyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spanish"
      ],
      "metadata": {
        "id": "xiDgSVgbGefs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate VADER predictions\n",
        "df_train_es[\"vader_prediction\"] = df_train_es[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_es['label'], df_train_es['vader_prediction'], digits=4))"
      ],
      "metadata": {
        "id": "IoU3dFFjGen8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### German"
      ],
      "metadata": {
        "id": "ih-49NXMGh0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate VADER predictions\n",
        "df_train_de[\"vader_prediction\"] = df_train_de[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_de['label'], df_train_de['vader_prediction'], digits=4))"
      ],
      "metadata": {
        "id": "KHfd7WHyGh-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### French"
      ],
      "metadata": {
        "id": "bhhEy_-YGmrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate VADER predictions\n",
        "df_train_fr[\"vader_prediction\"] = df_train_fr[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train_fr['label'], df_train_fr['vader_prediction'], digits=4))"
      ],
      "metadata": {
        "id": "qso6EAtCGkry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Training Set"
      ],
      "metadata": {
        "id": "2Eb9Npw7Gq2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate VADER predictions\n",
        "df_train[\"vader_prediction\"] = df_train[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_train['label'], df_train['vader_prediction'], digits=4))"
      ],
      "metadata": {
        "id": "g31YGjmjGrAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Set"
      ],
      "metadata": {
        "id": "0kCm-cz1G0r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# generate VADER predictions\n",
        "df_test[\"vader_prediction\"] = df_test[\"text\"].apply(predict_sentiment)\n",
        "\n",
        "labels = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
        "df_test['label'] = df_test['label'].map(labels)\n",
        "\n",
        "# Show the classification report\n",
        "print(classification_report(df_test['label'], df_test['vader_prediction'], digits=4))"
      ],
      "metadata": {
        "id": "JQ38-hBkG00W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cf_matrix2 = confusion_matrix(df_test['label'], df_test['vader_prediction'], labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "fig, ax = plt.subplots(figsize=(7,5))         # Sample figsize in inches\n",
        "sns.heatmap(cf_matrix2, annot=True, fmt='.0f', cmap='Blues', linewidths=.5, ax=ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.xaxis.set_ticklabels([\"negative\", \"neutral\", \"positive\"])\n",
        "ax.yaxis.set_ticklabels([\"negative\", \"neutral\", \"positive\"])"
      ],
      "metadata": {
        "id": "8W9iK1JQG96G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLM-R"
      ],
      "metadata": {
        "id": "tWUDVcZ6DfAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages and load data"
      ],
      "metadata": {
        "id": "y7CZE3CZHV22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor sentencepiece vader-multi spacy huggingface_hub transformers==4.28.0 datasets optuna"
      ],
      "metadata": {
        "id": "53L_J76AHV23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-Annotated-2023.xlsx\"\n",
        "df = pd.read_excel(dataset)"
      ],
      "metadata": {
        "id": "-9HTmqhNHV24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "CdTnb2UaHV24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df['tweet'] = df.apply(preprocess_tweets, axis=1)\n",
        "df.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df = df.rename(columns={'tweet': 'text', 'sentiment': 'label'})\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "labels = {-1: 0, 0: 1, 1: 2}\n",
        "df['label'] = df['label'].map(labels)"
      ],
      "metadata": {
        "id": "5Ms7jZAsHV25"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Validation-Test-Split"
      ],
      "metadata": {
        "id": "Bw_GU5skHF86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define independent/dependent variable\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# split with stratify on language and label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pd.concat([X[\"language\"], y], axis=1), stratify=pd.concat([X[\"language\"], y], axis=1), test_size=0.1, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_test = y_test.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "# val dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, pd.concat([X_train[\"language\"], y_train], axis=1), stratify=pd.concat([X_train[\"language\"], y_train], axis=1), test_size=1/9, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_val = y_val.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_val = pd.concat([X_val, y_val], axis=1)"
      ],
      "metadata": {
        "id": "jXE7RxHWHGeb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "BW43pMlYHlaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following fine-tuning process is done with the Trainer class provided through the transformer library. The code has been adapted from the official Hugging Face website (https://huggingface.co/docs/transformers/training)."
      ],
      "metadata": {
        "id": "BNmepY4hs7CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "datasets_train_val = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"val\": Dataset.from_pandas(df_val)\n",
        "    })\n",
        "\n",
        "#tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#preprocess\n",
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = datasets_train_val['train'].map(preprocess_function, batched=True)\n",
        "tokenized_val = datasets_train_val['val'].map(preprocess_function, batched=True)\n",
        "\n",
        "# metrics\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "   return {\"f1\": f1}\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-xlmr-unfreeze-last-2-best\""
      ],
      "metadata": {
        "id": "pFvLuIYsHliP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "4973e2dc5cf24327911442e1a0cdaab0",
            "54ffeacc0d6447ef8f82a2221a0f5bfc",
            "3998596af864468db6b6076aec544b49",
            "14aaf89973104cd58cbb5c6b749bec6e",
            "75527beca0a34825ab715721583ee4d9",
            "ca19b4fda90b43e699dbb4a4da73501f",
            "4bf2f493de6a492ca70dced9c5c58202",
            "5cc5d058ca21431f8fa06fecf5b7d4ac",
            "bbc0fdebf2f746b38f72489754d04919",
            "14cb7a122c744252b6d5aa2d96b26b44",
            "7bb2d65a46ed4214989f0bc36437aca9",
            "78b6bf7480af40618f60f7c7235777e8",
            "7870c888b9a948a98158aa1d554b9fad",
            "69d1322d14cf4108a65c2f188f933775",
            "7988d53aa993475e99f9f2644ce6362e",
            "62f6601ff87542329d18acc2bf99aba8",
            "ff02d879084f436d993b7cfb6335e980",
            "a6cbfd5bede3490e99c3e58a645d84e1",
            "ce991c76749742cdb110a732b24c6282",
            "41b96e3cce204359853767f32ac6f2ab",
            "fc30c586b1e5408ebf958e2219dc443e",
            "6cefaea9620a4479a9847969e8cee21e"
          ]
        },
        "outputId": "3fa3b2b5-32c4-4176-c056-4b18843de028"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4973e2dc5cf24327911442e1a0cdaab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78b6bf7480af40618f60f7c7235777e8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "WJlQL5ePIJzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze/unfreeze layers\n",
        "def model_init():\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
        "  for name, param in model.named_parameters():\n",
        "      if name.startswith(\"roberta.encoder.layer.0\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.1.\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.2\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.3\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.4\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.5\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.6\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.7\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.8\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.9\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.10\"):\n",
        "          param.requires_grad = True\n",
        "      if name.startswith(\"roberta.encoder.layer.11\"):\n",
        "          param.requires_grad = True\n",
        "  return model"
      ],
      "metadata": {
        "id": "keE7r0ZKH1vV"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  learning_rate=5.9e-5,\n",
        "  per_device_train_batch_size=32,\n",
        "  per_device_eval_batch_size=32,\n",
        "  num_train_epochs=10,\n",
        "  #push_to_hub=True,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  logging_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"f1\")"
      ],
      "metadata": {
        "id": "DBwmWNV3IAzK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "   model_init=model_init,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_val,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Rs6_a2iQIUvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "fLMvSJb6IYXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "CDqKMGB1IcJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune with all languages and evaluate on validation set"
      ],
      "metadata": {
        "id": "YAZy_qE1Ienu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_val)\n",
        "predictions = list(np.argmax(predictions.predictions, axis=-1))\n",
        "df_val['predictions_all_languages'] = predictions\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(df_val['label'], df_val['predictions_all_languages'], average='macro')"
      ],
      "metadata": {
        "id": "NhBk1WLCIex6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_all_en = df_val[df_val['language']=='en']\n",
        "tuned_all_es = df_val[df_val['language']=='es']\n",
        "tuned_all_de = df_val[df_val['language']=='de']\n",
        "tuned_all_fr = df_val[df_val['language']=='fr']"
      ],
      "metadata": {
        "id": "D_4Fxie6IrQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# English\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(classification_report(tuned_all_en['label'], tuned_all_en['predictions_all_languages'], digits=4))"
      ],
      "metadata": {
        "id": "QyEUSiY9I29N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spanish\n",
        "print(classification_report(tuned_all_es['label'], tuned_all_es['predictions_all_languages'], digits=4))"
      ],
      "metadata": {
        "id": "o9h_vsvAI7zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# German\n",
        "print(classification_report(tuned_all_de['label'], tuned_all_de['predictions_all_languages'], digits=4))"
      ],
      "metadata": {
        "id": "TvCkeTS6I72Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# French\n",
        "print(classification_report(tuned_all_fr['label'], tuned_all_fr['predictions_all_languages'], digits=4))"
      ],
      "metadata": {
        "id": "sI2e_Sj5I742"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune only with English tweets and evaluate on validation set"
      ],
      "metadata": {
        "id": "g5HmbQE2JDX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same code as above\n",
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-Annotated-2023.xlsx\"\n",
        "df = pd.read_excel(dataset)\n",
        "\n",
        "# pre-processing\n",
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df['tweet'] = df.apply(preprocess_tweets, axis=1)\n",
        "df.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df = df.rename(columns={'tweet': 'text', 'sentiment': 'label'})\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "labels = {-1: 0, 0: 1, 1: 2}\n",
        "df['label'] = df['label'].map(labels)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define independent/dependent variable\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# split with stratify on language and label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pd.concat([X[\"language\"], y], axis=1), stratify=pd.concat([X[\"language\"], y], axis=1), test_size=0.1, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_test = y_test.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "# val dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, pd.concat([X_train[\"language\"], y_train], axis=1), stratify=pd.concat([X_train[\"language\"], y_train], axis=1), test_size=1/9, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_val = y_val.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_val = pd.concat([X_val, y_val], axis=1)\n",
        "\n",
        "df_train = df_train[df_train['language']=='en']\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "#df_train = Dataset.from_pandas(df_train)\n",
        "#df_val = Dataset.from_pandas(df_val)\n",
        "\n",
        "datasets_train_val = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"val\": Dataset.from_pandas(df_val)\n",
        "    })\n",
        "\n",
        "#tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#preprocess\n",
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = datasets_train_val['train'].map(preprocess_function, batched=True)\n",
        "tokenized_val = datasets_train_val['val'].map(preprocess_function, batched=True)\n",
        "\n",
        "#from transformers import DataCollatorWithPadding\n",
        "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# metrics\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   #load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   #accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "   #return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "   return {\"f1\": f1}\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-xlmr-unfreeze-last-2-best\"\n",
        "\n",
        "def model_init():\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
        "  for name, param in model.named_parameters():\n",
        "      if name.startswith(\"roberta.encoder.layer.0\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.1.\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.2\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.3\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.4\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.5\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.6\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.7\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.8\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.9\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.10\"):\n",
        "          param.requires_grad = True\n",
        "      if name.startswith(\"roberta.encoder.layer.11\"):\n",
        "          param.requires_grad = True\n",
        "  return model\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  learning_rate=1e-4,\n",
        "  per_device_train_batch_size=32,\n",
        "  per_device_eval_batch_size=32,\n",
        "  num_train_epochs=10,\n",
        "  #push_to_hub=True,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  logging_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"f1\")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model_init=model_init,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_val,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "hdnuJER7JHtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_y5HhCGqJtjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "AmL_g29tJtjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_val)\n",
        "predictions = list(np.argmax(predictions.predictions, axis=-1))\n",
        "df_val['predictions_only_english'] = predictions\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(df_val['label'], df_val['predictions_only_english'], average='macro')"
      ],
      "metadata": {
        "id": "b4Phc2gTvFgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_onlyen_en = df_val[df_val['language']=='en']\n",
        "tuned_onlyen_es = df_val[df_val['language']=='es']\n",
        "tuned_onlyen_de = df_val[df_val['language']=='de']\n",
        "tuned_onlyen_fr = df_val[df_val['language']=='fr']"
      ],
      "metadata": {
        "id": "npy7oTLIvFi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(classification_report(tuned_onlyen_en['label'], tuned_onlyen_en['predictions_only_english'], digits=4))"
      ],
      "metadata": {
        "id": "UixwS3KIwOqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(tuned_onlyen_es['label'], tuned_onlyen_es['predictions_only_english'], digits=4))"
      ],
      "metadata": {
        "id": "lnrvjIGowOqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(tuned_onlyen_de['label'], tuned_onlyen_de['predictions_only_english'], digits=4))"
      ],
      "metadata": {
        "id": "t672BAKswOqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(tuned_onlyen_fr['label'], tuned_onlyen_fr['predictions_only_english'], digits=4))"
      ],
      "metadata": {
        "id": "gsJqmr_mwOqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter-tuning"
      ],
      "metadata": {
        "id": "H9uxSVZJKtFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same code as above\n",
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-Annotated-2023.xlsx\"\n",
        "df = pd.read_excel(dataset)\n",
        "\n",
        "# pre-processing\n",
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df['tweet'] = df.apply(preprocess_tweets, axis=1)\n",
        "df.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df = df.rename(columns={'tweet': 'text', 'sentiment': 'label'})\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "labels = {-1: 0, 0: 1, 1: 2}\n",
        "df['label'] = df['label'].map(labels)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define independent/dependent variable\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# split with stratify on language and label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pd.concat([X[\"language\"], y], axis=1), stratify=pd.concat([X[\"language\"], y], axis=1), test_size=0.1, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_test = y_test.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "# val dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, pd.concat([X_train[\"language\"], y_train], axis=1), stratify=pd.concat([X_train[\"language\"], y_train], axis=1), test_size=1/9, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_val = y_val.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_val = pd.concat([X_val, y_val], axis=1)\n",
        "\n",
        "df_train = df_train[df_train['language']=='en']\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "#df_train = Dataset.from_pandas(df_train)\n",
        "#df_val = Dataset.from_pandas(df_val)\n",
        "\n",
        "datasets_train_val = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"val\": Dataset.from_pandas(df_val)\n",
        "    })\n",
        "\n",
        "#tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#preprocess\n",
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = datasets_train_val['train'].map(preprocess_function, batched=True)\n",
        "tokenized_val = datasets_train_val['val'].map(preprocess_function, batched=True)\n",
        "\n",
        "#from transformers import DataCollatorWithPadding\n",
        "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# metrics\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   #load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   #accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "   #return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "   return {\"f1\": f1}\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-xlmr-unfreeze-last-2-best\"\n",
        "\n",
        "def model_init():\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
        "  for name, param in model.named_parameters():\n",
        "      if name.startswith(\"roberta.encoder.layer.0\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.1.\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.2\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.3\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.4\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.5\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.6\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.7\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.8\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.9\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.10\"):\n",
        "          param.requires_grad = True\n",
        "      if name.startswith(\"roberta.encoder.layer.11\"):\n",
        "          param.requires_grad = True\n",
        "  return model\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  learning_rate=1e-4,\n",
        "  per_device_train_batch_size=32,\n",
        "  per_device_eval_batch_size=32,\n",
        "  num_train_epochs=10,\n",
        "  #push_to_hub=True,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  logging_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"f1\")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model_init=model_init,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_val,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "HPEPzh39Kv3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter-tuning\n",
        "def optuna_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True),\n",
        "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
        "        \"num_train_epochs\": 10\n",
        "    }"
      ],
      "metadata": {
        "id": "TJhTDv-yLa82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = trainer.hyperparameter_search(n_trials=16, direction=\"maximize\", hp_space=optuna_hp_space)\n",
        "best_run"
      ],
      "metadata": {
        "id": "b2ox-z43LhaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation on the test set"
      ],
      "metadata": {
        "id": "GXpHf0PxLrvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor sentencepiece vader-multi spacy huggingface_hub transformers==4.28.0 datasets optuna"
      ],
      "metadata": {
        "id": "zh5xKsBDQ77r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same code as above. Repeat for different random_state when splitting and take average.\n",
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-Annotated-2023.xlsx\"\n",
        "df = pd.read_excel(dataset)\n",
        "\n",
        "# pre-processing\n",
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df['tweet'] = df.apply(preprocess_tweets, axis=1)\n",
        "df.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df = df.rename(columns={'tweet': 'text', 'sentiment': 'label'})\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "labels = {-1: 0, 0: 1, 1: 2}\n",
        "df['label'] = df['label'].map(labels)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define independent/dependent variable\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# split with stratify on language and label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pd.concat([X[\"language\"], y], axis=1), stratify=pd.concat([X[\"language\"], y], axis=1), test_size=0.1, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_test = y_test.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "#df_train = Dataset.from_pandas(df_train)\n",
        "#df_val = Dataset.from_pandas(df_val)\n",
        "\n",
        "datasets_train_test = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"test\": Dataset.from_pandas(df_test)\n",
        "    })\n",
        "\n",
        "#tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#preprocess\n",
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = datasets_train_test['train'].map(preprocess_function, batched=True)\n",
        "tokenized_test = datasets_train_test['test'].map(preprocess_function, batched=True)\n",
        "\n",
        "#from transformers import DataCollatorWithPadding\n",
        "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# metrics\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   #load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   #accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "   #return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "   return {\"f1\": f1}\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-xlmr-unfreeze-last-2-best\"\n",
        "\n",
        "def model_init():\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
        "  for name, param in model.named_parameters():\n",
        "      if name.startswith(\"roberta.encoder.layer.0\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.1.\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.2\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.3\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.4\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.5\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.6\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.7\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.8\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.9\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.10\"):\n",
        "          param.requires_grad = True\n",
        "      if name.startswith(\"roberta.encoder.layer.11\"):\n",
        "          param.requires_grad = True\n",
        "  return model\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  learning_rate=5.9e-5,\n",
        "  per_device_train_batch_size=32,\n",
        "  per_device_eval_batch_size=32,\n",
        "  num_train_epochs=10,\n",
        "  #push_to_hub=True,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  logging_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"f1\")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model_init=model_init,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "NXwP-_zmLr3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MDq3WDVhL1Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "fa0UoFc5skRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "predictions = list(np.argmax(predictions.predictions, axis=-1))\n",
        "df_test['predictions'] = predictions\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(df_test['label'], df_test['predictions'], average='macro')"
      ],
      "metadata": {
        "id": "J5SER020L3T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(classification_report(df_test['label'], df_test['predictions'], digits=4))"
      ],
      "metadata": {
        "id": "oaFib0rUL8Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "df_test['label'] = df_test['label'].map(labels)\n",
        "df_test['predictions'] = df_test['predictions'].map(labels)\n",
        "\n",
        "cf_matrix = confusion_matrix(df_test['label'], df_test['predictions'], labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "fig, ax = plt.subplots(figsize=(7,5))         # Sample figsize in inches\n",
        "sns.heatmap(cf_matrix, annot=True, fmt='.0f', cmap='Blues', linewidths=.5, ax=ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.xaxis.set_ticklabels([\"negative\", \"neutral\", \"positive\"])\n",
        "ax.yaxis.set_ticklabels([\"negative\", \"neutral\", \"positive\"])"
      ],
      "metadata": {
        "id": "Th3x9VLeL956"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply on the whole dataset - RQ 4"
      ],
      "metadata": {
        "id": "WW_psWXNTtkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor sentencepiece vader-multi spacy huggingface_hub transformers==4.28.0 datasets optuna"
      ],
      "metadata": {
        "id": "I5zzArpvT1GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same code as above. Repeat for different random_state when splitting and take average.\n",
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-Annotated-2023.xlsx\"\n",
        "df = pd.read_excel(dataset)\n",
        "\n",
        "# pre-processing\n",
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df['tweet'] = df.apply(preprocess_tweets, axis=1)\n",
        "df.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df = df.rename(columns={'tweet': 'text', 'sentiment': 'label'})\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "labels = {-1: 0, 0: 1, 1: 2}\n",
        "df['label'] = df['label'].map(labels)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# define independent/dependent variable\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# split with stratify on language and label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, pd.concat([X[\"language\"], y], axis=1), stratify=pd.concat([X[\"language\"], y], axis=1), test_size=0.1, random_state=1)\n",
        "\n",
        "# remove language from target variable\n",
        "y_train = y_train.drop([\"language\"], axis=1)\n",
        "y_test = y_test.drop([\"language\"], axis=1)\n",
        "\n",
        "# concat dataframes back together\n",
        "df_train = pd.concat([X_train, y_train], axis=1)\n",
        "df_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "#df_train = Dataset.from_pandas(df_train)\n",
        "#df_val = Dataset.from_pandas(df_val)\n",
        "\n",
        "datasets_train_test = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train),\n",
        "    \"test\": Dataset.from_pandas(df_test)\n",
        "    })\n",
        "\n",
        "#tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#preprocess\n",
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = datasets_train_test['train'].map(preprocess_function, batched=True)\n",
        "tokenized_test = datasets_train_test['test'].map(preprocess_function, batched=True)\n",
        "\n",
        "#from transformers import DataCollatorWithPadding\n",
        "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# metrics\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   #load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   #accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "   #return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "   return {\"f1\": f1}\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-xlmr-unfreeze-last-2-best\"\n",
        "\n",
        "def model_init():\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=3)\n",
        "  for name, param in model.named_parameters():\n",
        "      if name.startswith(\"roberta.encoder.layer.0\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.1.\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.2\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.3\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.4\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.5\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.6\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.7\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.8\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.9\"):\n",
        "          param.requires_grad = False\n",
        "      if name.startswith(\"roberta.encoder.layer.10\"):\n",
        "          param.requires_grad = True\n",
        "      if name.startswith(\"roberta.encoder.layer.11\"):\n",
        "          param.requires_grad = True\n",
        "  return model\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=repo_name,\n",
        "  learning_rate=5.9e-5,\n",
        "  per_device_train_batch_size=32,\n",
        "  per_device_eval_batch_size=32,\n",
        "  num_train_epochs=10,\n",
        "  #push_to_hub=True,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  logging_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"f1\")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model_init=model_init,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "S1df5PQWTxuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9cesPT2yT5iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "AlNzlrqYUKnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# whole dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "dataset_all = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/AltProteinTweets-All-2023.csv\"\n",
        "df_all = pd.read_csv(dataset_all)\n",
        "\n",
        "# pre-processing\n",
        "import preprocessor as p\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION)\n",
        "def preprocess_tweets(tweet):\n",
        "    text = tweet['tweet']\n",
        "    text = p.clean(text)\n",
        "    return text.replace('#','')\n",
        "\n",
        "df_all['tweet'] = df_all.apply(preprocess_tweets, axis=1)\n",
        "df_all.drop(columns=[\"tweetid\", \"datecreated\", \"qoutedTweet\", \"User\"], inplace=True)\n",
        "df_all = df_all.rename(columns={'tweet': 'text'})\n",
        "df_all = df_all.reset_index(drop=True)\n",
        "\n",
        "#labels = {-1: 0, 0: 1, 1: 2}\n",
        "#df_all['label'] = df_all['label'].map(labels)\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "#df_train = Dataset.from_pandas(df_train)\n",
        "#df_val = Dataset.from_pandas(df_val)\n",
        "\n",
        "datasets_all = DatasetDict({\n",
        "    \"all\": Dataset.from_pandas(df_all)\n",
        "    })\n",
        "\n",
        "#tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "#preprocess\n",
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_df_all = datasets_all['all'].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "2P6KsVMdUOZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "predictions = trainer.predict(tokenized_df_all)\n",
        "predictions = list(np.argmax(predictions.predictions, axis=-1))\n",
        "df_all['predictions'] = predictions"
      ],
      "metadata": {
        "id": "t_LCcKhtUMWo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "54004a82-ee7d-43e6-a6d5-1dc05981d867"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_rq4 = \"/content/drive/MyDrive/Nams Surface/Desktop - Backup/Uni Tilburg/Master Thesis/Code/dataset_all_predictions.csv\"\n",
        "df_rq4 = pd.read_csv(dataset_rq4)"
      ],
      "metadata": {
        "id": "GkDuAgMb95zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# absolute values\n",
        "pd.crosstab(df_rq4['language'], df_rq4['predictions'], margins=True)"
      ],
      "metadata": {
        "id": "DfIDmZMtzH6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# relative values\n",
        "round(pd.crosstab(df_rq4['language'], df_rq4['predictions'], margins=True, normalize='index')*100)"
      ],
      "metadata": {
        "id": "LG8M5EUxzKZX"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}